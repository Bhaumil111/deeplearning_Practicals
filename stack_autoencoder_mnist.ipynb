{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Basic code for MNIST classification where for feature engineering stacked vanilla autoencoder is used.\n- There are many optimization possible pertaining to architecture, training and hyperparameter tuning. \n- We can also run it for more epochs.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nfrom torchvision.transforms import ToTensor\nfrom matplotlib import pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-02T11:59:05.668530Z","iopub.execute_input":"2025-04-02T11:59:05.669312Z","iopub.status.idle":"2025-04-02T11:59:05.674035Z","shell.execute_reply.started":"2025-04-02T11:59:05.669280Z","shell.execute_reply":"2025-04-02T11:59:05.672907Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device(type='cuda',index=0)\nelse:\n    device=torch.device(type='cpu',index=0)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.675733Z","iopub.execute_input":"2025-04-02T11:59:05.676098Z","iopub.status.idle":"2025-04-02T11:59:05.752795Z","shell.execute_reply.started":"2025-04-02T11:59:05.676076Z","shell.execute_reply":"2025-04-02T11:59:05.751846Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=True,\n                            download=True,\n                            transform=ToTensor())\n\neval_dataset=datasets.MNIST(root=\"/kaggle/temp/\",\n                            train=False,\n                            download=True,\n                            transform=ToTensor())\n\nbatch_size=64\n\ntrain_dataloader=DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\neval_dataloader=DataLoader(dataset=eval_dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.754257Z","iopub.execute_input":"2025-04-02T11:59:05.754680Z","iopub.status.idle":"2025-04-02T11:59:05.871734Z","shell.execute_reply.started":"2025-04-02T11:59:05.754647Z","shell.execute_reply":"2025-04-02T11:59:05.870976Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Encoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=784,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=256)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.lkrelu(net2)\n        return out2\n    \nclass Decoder1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.sig=nn.Sigmoid()\n        self.l1=nn.Linear(in_features=256,out_features=512)\n        self.l2=nn.Linear(in_features=512,out_features=784)\n    \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        net2=self.l2(out1)\n        out2=self.sig(net2)\n        return out2\n\nclass Encoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=256,out_features=100)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n    \nclass Decoder2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lkrelu=nn.LeakyReLU(0.1)\n        self.l1=nn.Linear(in_features=100,out_features=256)\n            \n    def forward(self,x):\n        net1=self.l1(x)\n        out1=self.lkrelu(net1)\n        return out1\n\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.out=nn.Linear(in_features=100,out_features=10)\n    \n    def forward(self,x):\n        out=self.out(x)\n        return out","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.872951Z","iopub.execute_input":"2025-04-02T11:59:05.873246Z","iopub.status.idle":"2025-04-02T11:59:05.883787Z","shell.execute_reply.started":"2025-04-02T11:59:05.873225Z","shell.execute_reply":"2025-04-02T11:59:05.882692Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"lossmse_fn=nn.MSELoss()\nlossentropy_fn=nn.CrossEntropyLoss()\nlr=0.001\n\ne1=Encoder1().to(device)\nd1=Decoder1().to(device)\ne2=Encoder2().to(device)\nd2=Decoder2().to(device)\nclf=Classifier().to(device)\n\nopte1=Adam(params=e1.parameters(),lr=lr)\noptd1=Adam(params=d1.parameters(),lr=lr)\nopte2=Adam(params=e2.parameters(),lr=lr)\noptd2=Adam(params=d2.parameters(),lr=lr)\noptclf=Adam(params=clf.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.886394Z","iopub.execute_input":"2025-04-02T11:59:05.886696Z","iopub.status.idle":"2025-04-02T11:59:05.912595Z","shell.execute_reply.started":"2025-04-02T11:59:05.886669Z","shell.execute_reply":"2025-04-02T11:59:05.911883Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"#train AutoEncoder1\ndef train_e1d1():    \n    track_loss=0\n    \n    e1.train()\n    d1.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latent=e1(x)\n        pred=d1(latent)\n            \n        loss=lossmse_fn(pred,x)\n        \n        track_loss+=loss.item()\n          \n        loss.backward()\n        \n        opte1.step()\n        optd1.step()\n        \n        opte1.zero_grad()\n        optd1.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    return round(running_loss,4)\n\n#train AutoEncoder2\ndef train_e2d2():    \n    track_loss=0\n    \n    e2.train()\n    d2.train()\n    \n    for i,(x,_) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1.detach())\n        pred=d2(latente2)\n            \n        loss=lossmse_fn(pred,latente1.detach()) \n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte2.step()\n        optd2.step()\n        opte2.zero_grad()\n        optd2.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n\n#train Classifier\ndef train_clf():  \n    track_loss=0\n    num_correct=0\n    \n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2.detach())\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        optclf.step()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#finetune whole \ndef train_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.train()\n    e2.train()\n    clf.train()\n    \n    for i,(x,y) in enumerate(train_dataloader):\n        x=torch.reshape(x,shape=(-1,784))\n        x=x.to(device)\n        y=y.to(device)\n\n        latente1=e1(x)\n        latente2=e2(latente1)\n        pred=clf(latente2)\n        \n        num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            \n        loss=lossentropy_fn(pred,y)\n        \n        track_loss+=loss.item()\n        \n        loss.backward()\n        \n        opte1.step()\n        opte2.step()\n        optclf.step()\n        \n        opte1.zero_grad()\n        opte2.zero_grad()\n        optclf.zero_grad()\n        \n        running_loss=track_loss/(i+(x.shape[0]/batch_size))\n        running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n    \n    return round(running_loss,4),round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.913539Z","iopub.execute_input":"2025-04-02T11:59:05.913813Z","iopub.status.idle":"2025-04-02T11:59:05.928660Z","shell.execute_reply.started":"2025-04-02T11:59:05.913788Z","shell.execute_reply":"2025-04-02T11:59:05.927728Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#Eval AutoEncoder1\ndef eval_e1d1():\n    track_loss=0\n    \n    e1.eval()\n    d1.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latent=e1(x)\n            pred=d1(latent)\n\n            loss=lossmse_fn(pred,x)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4),x,pred\n\n#Eval AutoEncoder2\ndef eval_e2d2():\n    track_loss=0\n    \n    e2.eval()\n    d2.eval()\n    \n    with torch.no_grad():\n        for i,(x,_) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=d2(latente2)\n\n            loss=lossmse_fn(pred,latente1)\n            \n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4)\n            \n        \n#Eval Classifier\ndef eval_clf():\n    track_loss=0\n    num_correct=0\n    \n    clf.eval()\n    \n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n            \n    \n    return round(running_loss,4), round(running_acc,2)\n        \n#eval whole \ndef eval_whole():\n    track_loss=0\n    num_correct=0\n    \n    e1.eval()\n    e2.eval()\n    clf.eval()\n    with torch.no_grad():\n        for i,(x,y) in enumerate(eval_dataloader):\n            x=torch.reshape(x,shape=(-1,784))\n            x=x.to(device)\n            y=y.to(device)\n\n            latente1=e1(x)\n            latente2=e2(latente1)\n            pred=clf(latente2)\n            \n            num_correct+=(torch.argmax(pred,dim=1)==y).type(torch.float).sum().item()\n            running_acc=(num_correct/((i*batch_size+x.shape[0])))*100\n            \n            loss=lossentropy_fn(pred,y)\n            track_loss+=loss.item()\n            \n            running_loss=track_loss/(i+(x.shape[0]/batch_size))\n    \n    return round(running_loss,4), round(running_acc,2)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.929942Z","iopub.execute_input":"2025-04-02T11:59:05.930218Z","iopub.status.idle":"2025-04-02T11:59:05.948168Z","shell.execute_reply.started":"2025-04-02T11:59:05.930197Z","shell.execute_reply":"2025-04-02T11:59:05.947159Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"n_epochs=2\n\nprint(\"-----------------------Auto Encoder 1------------------------\")\nfor i in range(n_epochs):\n    train_loss=train_e1d1()\n    eval_loss,x,pred=eval_e1d1()\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_loss,\", Eval Loss=\",eval_loss,sep=\"\")\n    \nplt.figure(figsize=(3.2,2.4))\n\nr=torch.randint(low=0,high=pred.shape[0],size=(1,)).item()\n\nplt.subplot(1,2,1)\nplt.title(\"Orignal\")\nplt.imshow(torch.reshape(x[r],shape=(28,28)).cpu())\n\nplt.subplot(1,2,2)\nplt.title(\"Prediction\")\nplt.imshow(torch.reshape(pred[r],shape=(28,28)).cpu())\nplt.show()\n    \nprint(\"-----------------------Auto Encoder 2------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss=\",train_e2d2(),\", Eval Loss=\",eval_e2d2(),sep=\"\") \n\n\nprint(\"-----------------------Classifier Only------------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_clf(),\", Eval Loss & Accuracy=\",eval_clf(),sep=\"\") \n\n\nprint(\"--------------------Fine-Tuning Whole Metwork------------------\")\nfor i in range(n_epochs):\n    print(\"Epoch=\", i+1,\", Train Loss & Accuracy=\",train_whole(),\", Eval Loss & Accuracy=\",eval_whole(),sep=\"\") \n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T11:59:05.949467Z","iopub.execute_input":"2025-04-02T11:59:05.949847Z","iopub.status.idle":"2025-04-02T12:00:16.875523Z","shell.execute_reply.started":"2025-04-02T11:59:05.949789Z","shell.execute_reply":"2025-04-02T12:00:16.874583Z"},"trusted":true},"outputs":[{"name":"stdout","text":"-----------------------Auto Encoder 1------------------------\nEpoch=1, Train Loss=0.0221, Eval Loss=0.0081\nEpoch=2, Train Loss=0.0067, Eval Loss=0.0055\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 320x240 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"-----------------------Auto Encoder 2------------------------\nEpoch=1, Train Loss=0.2884, Eval Loss=0.0272\nEpoch=2, Train Loss=0.0203, Eval Loss=0.0163\n-----------------------Classifier Only------------------------\nEpoch=1, Train Loss & Accuracy=(0.8494, 75.46), Eval Loss & Accuracy=(0.414, 89.0)\nEpoch=2, Train Loss & Accuracy=(0.398, 88.81), Eval Loss & Accuracy=(0.3429, 90.47)\n--------------------Fine-Tuning Whole Metwork------------------\nEpoch=1, Train Loss & Accuracy=(0.1895, 94.27), Eval Loss & Accuracy=(0.1078, 96.53)\nEpoch=2, Train Loss & Accuracy=(0.0783, 97.56), Eval Loss & Accuracy=(0.0786, 97.48)\n","output_type":"stream"}],"execution_count":17}]}